{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import random_split\n",
    "from torchvision.transforms import ToTensor, Normalize, Compose, Resize\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import sys\n",
    "sys.path.append('../../')\n",
    "from Data_preparation.heatmaps import coord2Heatmap\n",
    "from IPython.display import clear_output\n",
    "import time\n",
    "\n",
    "\n",
    "class LandmarkDataset(Dataset):\n",
    "    def __init__(self, image_paths, cam_points, visibility_values, image_transform, heatmap_transform):\n",
    "        self.image_paths = image_paths\n",
    "        self.cam_points = cam_points\n",
    "        self.visibility_values = visibility_values\n",
    "\n",
    "        self.image_transform = image_transform\n",
    "        self.heatmap_transform = heatmap_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path = self.image_paths[idx]\n",
    "        image = Image.open(image_path).convert('L')\n",
    "        image = self.image_transform(image)\n",
    "\n",
    "        cam_points = self.cam_points[idx]\n",
    "        heatmaps = coord2Heatmap(cam_points, visibility[idx])\n",
    "        heatmaps = self.heatmap_transform(heatmaps)\n",
    "\n",
    "        visibility = torch.tensor(self.visibility_values[idx])\n",
    "\n",
    "        return image, heatmaps, visibility\n",
    "    \n",
    "def select_data(dataset, n_landmarks=9, image_width=512, image_height=512):\n",
    "    data = pd.read_csv(dataset)\n",
    "    LDM_pos = []\n",
    "    for i in range(n_landmarks):\n",
    "        LDM_pos.append(f'LDM{i}x')\n",
    "        LDM_pos.append(f'LDM{i}y')\n",
    "        \n",
    "    Vp = [f'VP{i}' for i in range(n_landmarks)]\n",
    "\n",
    "    data = data[data['image_nr'] < 800]\n",
    "\n",
    "    image_paths = data['image_path'].values\n",
    "    cam_points = data[LDM_pos].values.reshape(-1, n_landmarks, 2)\n",
    "    visibility = data[Vp].values\n",
    "\n",
    "    return image_paths, cam_points, visibility\n",
    "\n",
    "def load_data(dataset, n_landmarks=9, batch_size=4, image_width=512, image_height=512):\n",
    "    for data in dataset:\n",
    "        img, cam_p, vis = select_data(data, n_landmarks, image_width, image_height)\n",
    "        if data == dataset[0]:\n",
    "            image_path = img\n",
    "            cam_points = cam_p\n",
    "            visibility = vis\n",
    "        else:\n",
    "            image_paths = np.concatenate((image_path, img))\n",
    "            cam_points = np.concatenate((cam_points, cam_p))\n",
    "            visibility = np.concatenate((visibility, vis))\n",
    "\n",
    "    input_transform = Compose([\n",
    "        Resize((image_width, image_height)),\n",
    "        ToTensor(),\n",
    "        Normalize(mean=[0.5], std=[0.5])\n",
    "    ])\n",
    "    target_transform = Compose([\n",
    "        Resize((image_width, image_height)),\n",
    "        ToTensor(),\n",
    "        Normalize(mean=[0.5], std=[0.5])\n",
    "    ])\n",
    "\n",
    "    dataset = LandmarkDataset(image_paths, cam_points, visibility, input_transform, target_transform)\n",
    "\n",
    "    train_size = int(0.8 * len(dataset))\n",
    "    test_size = len(dataset) - train_size\n",
    "\n",
    "    train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    return train_loader, test_loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load done\n"
     ]
    }
   ],
   "source": [
    "dataset_path = ['../../Data_preparation/DatasetN2_3.csv', '../../Data_preparation/DatasetN3_3.csv']\n",
    "\n",
    "train_loader, test_loader = load_data(dataset_path)\n",
    "print('Load done')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
